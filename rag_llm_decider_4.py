# -*- coding: utf-8 -*-
"""
RAG + hazard model + alarm policy + LLM Decision。

Dependencies：
- predict_risk_baseline_3.py
    - train:
        - hazard_model.pkl
        - hazard_model_features.json
- txt_rag_standalone.py
        - store_txt_rag/
            - texts.jsonl
            - metas.jsonl
            - vectorizer.pkl
            - X_tfidf.npz
            - E_dense.npy

Usage:
python rag_llm_decider_4.py \
    --corpus store_txt_rag \
    --question "Risk & guidance for current single-ventricle infant" \
    --patient-csv sim_ecmo_timeseries.csv \
    --time-col AR \
    --freq-sec 15 \
    --lead-min 15 \
    --horizon-min 60

Ollama + llama3.1:
python rag_llm_decider_4.py \
    --corpus store_txt_rag \
    --question "Risk & guidance for current single-ventricle infant" \
    --patient-csv sim_ecmo_timeseries.csv \
    --time-col AR \
    --freq-sec 15 \
    --lead-min 15 \
    --horizon-min 60 \
    --ollama-model "llama3.1:8b"
"""

from __future__ import annotations
import argparse
import json
import math
import os
import re
import shutil
import subprocess
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import joblib
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix, load_npz
from sklearn.feature_extraction.text import TfidfVectorizer

from predict_risk_baseline_3 import (
    build_features_single,
    THRESHOLDS_FALLBACK as THRESHOLDS,
    minutes_to_steps,
)

# ======================= CLI =======================
def get_args():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--corpus", required=True,
        help="Directory of the RAG index generated by txt_rag_standalone "
             "(must contain texts.jsonl and metas.jsonl)."
    )
    ap.add_argument(
        "--question", required=True,
        help="Clinical question to be answered, e.g. "
             "'Risk and guidance for current single-ventricle infant'."
    )
    ap.add_argument(
        "--patient-csv", required=True,
        help="Time-series CSV for a single patient, e.g. sim_ecmo_timeseries.csv."
    )

    ap.add_argument("--time-col", default="AR",
                    help="Name of the timestamp column in the patient CSV.")
    ap.add_argument("--freq-sec", type=int, default=15,
                    help="Sampling frequency in seconds (e.g. 15).")

    # Hazard / episode parameters (must match hazard_model training)
    ap.add_argument("--lead-min", type=int, default=15,
                    help="Lead time used when defining hazard labels (minutes).")
    ap.add_argument("--cooldown-min", type=int, default=30,
                    help="Cooldown between episodes when grouping events (minutes).")
    ap.add_argument("--horizon-min", type=int, default=60,
                    help="Future risk horizon to describe in the explanation (minutes).")

    # RAG retrieval
    ap.add_argument("--topk", type=int, default=6,
                    help="Number of guideline snippets to include in the LLM prompt.")
    ap.add_argument("--use-dense", action="store_true",
                    help="Fuse dense embeddings with TF-IDF if available (E_dense.npy or sentence-transformers).")

    # Alarm / smoothing policy
    ap.add_argument("--smooth-alpha", type=float, default=0.35,
                    help="EWMA smoothing factor in [0,1]; larger values track the current risk more closely.")
    ap.add_argument("--hold-min", type=float, default=1.0,
                    help="Minimum time (minutes) above threshold required to trigger an alarm.")
    ap.add_argument("--refractory-min", type=float, default=10.0,
                    help="Refractory period between two alarms (minutes).")

    # LLM
    ap.add_argument("--ollama-model", default="",
                    help="Ollama model name, e.g. 'llama3.1:8b'. "
                         "If empty, only template-based conclusions are generated.")

    return ap.parse_args()

# ======================= Utils =======================

def _now_year() -> int:
    return 2025

def _minmax01(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x, float)
    if x.size == 0:
        return x
    lo = np.nanmin(x)
    hi = np.nanmax(x)
    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:
        return np.zeros_like(x)
    return (x - lo) / (hi - lo)

def _cosine(a: np.ndarray, b: np.ndarray, eps: float = 1e-9) -> float:
    na = np.linalg.norm(a)
    nb = np.linalg.norm(b)
    if na < eps or nb < eps:
        return 0.0
    return float(np.dot(a, b) / (na * nb))

def try_ollama(prompt: str, model: str, timeout: int = 120) -> Optional[str]:
    if not model or shutil.which("ollama") is None:
        return None
    try:
        r = subprocess.run(
            ["ollama", "run", model],
            input=prompt.encode("utf-8"),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=timeout,
        )
        if r.returncode != 0:
            return None
        return r.stdout.decode("utf-8", "ignore")
    except Exception:
        return None

def extract_json(text: str) -> Optional[dict]:
    if not text:
        return None
    m = re.search(r"```json(.*?)```", text, flags=re.S | re.I)
    blob = m.group(1) if m else None
    if not blob:
        m2 = re.search(r"\{.*\}", text, flags=re.S)
        blob = m2.group(0) if m2 else None
    if not blob:
        return None
    try:
        return json.loads(blob)
    except Exception:
        blob2 = re.sub(r",\s*([}\]])", r"\1", blob)
        try:
            return json.loads(blob2)
        except Exception:
            return None

# ======================= RAG Retrieval =======================

@dataclass
class RetrievalConfig:
    token_pattern: str = r"(?u)\b[\w/%.+\-]+\b"
    ngram_range: Tuple[int, int] = (1, 2)
    min_df: int = 2
    max_df: float = 0.9
    max_features: int = 60000
    sublinear_tf: bool = True
    norm: str = "l2"

    rrf_k: int = 60
    variants_topk: int = 50
    adaptive_gamma: float = 0.20
    min_keep: int = 5
    max_keep: int = 20
    mmr_lambda: float = 0.7
    mmr_top_m: int = 8

    alpha_lex: float = 0.60
    beta_dense: float = 0.30
    lambda_prior: float = 0.10

    recency_tau: float = 6.0
    doc_type_weight: Dict[str, float] = field(default_factory=lambda: {
        "guideline": 1.0,
        "review": 0.7,
        "trial": 0.6,
        "cohort": 0.5,
        "case": 0.3,
        "other": 0.4,
    })
    pediatric_bonus: float = 0.15
    sv_bonus: float = 0.15

    synonyms: Dict[str, List[str]] = field(default_factory=lambda: {
        "oxygenation index": ["OI", "PaO2/FiO2", "PF ratio", "oxygenation"],
        "ecmo weaning": ["decannulation", "wean from ECMO", "discontinuation"],
        "indications": ["criteria", "thresholds", "initiation"],
        "lactate": ["serum lactate"],
        "nirs": ["near-infrared spectroscopy", "cerebral oximetry"],
        "single ventricle": ["single-ventricle physiology", "SV physiology"],
        "neonate": ["neonatal", "newborn", "infant"],
    })

def read_jsonl(p: Path) -> List[dict]:
    rows = []
    with p.open("r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                rows.append(json.loads(line))
    return rows

def _coerce_text_list(objs):
    texts = []
    for r in objs:
        if isinstance(r, str):
            texts.append(r)
        elif isinstance(r, dict):
            if "text" in r and isinstance(r["text"], str):
                texts.append(r["text"])
            elif "content" in r and isinstance(r["content"], str):
                texts.append(r["content"])
            elif "chunk" in r and isinstance(r["chunk"], str):
                texts.append(r["chunk"])
            else:
                texts.append(json.dumps(r, ensure_ascii=False))
        else:
            texts.append(str(r))
    return texts

def _coerce_meta_list(objs):
    metas = []
    for r in objs:
        if isinstance(r, dict):
            if "meta" in r and isinstance(r["meta"], dict):
                metas.append(dict(r["meta"]))
            else:
                metas.append(dict(r))
        else:
            metas.append({"source": "unknown", "note": str(r)})
    return metas

_HAS_ST = False
try:
    from sentence_transformers import SentenceTransformer
    _HAS_ST = True
except Exception:
    _HAS_ST = False

class RetrieverStandalone:
    def __init__(self, cfg: RetrievalConfig, use_dense: bool = True):
        self.cfg = cfg
        self.use_dense = bool(use_dense)
        self.texts: List[str] = []
        self.metas: List[Dict[str, Any]] = []
        self.vectorizer: Optional[TfidfVectorizer] = None
        self.X: Optional[csr_matrix] = None
        self.E: Optional[np.ndarray] = None
        self._dense_model: Optional[SentenceTransformer] = None

    def load_from_dir(self, d: Path) -> "RetrieverStandalone":
        t0 = time.time()
        texts_raw = read_jsonl(d / "texts.jsonl")
        metas_raw = read_jsonl(d / "metas.jsonl")

        self.texts = _coerce_text_list(texts_raw)
        self.metas = _coerce_meta_list(metas_raw)

        if len(self.texts) != len(self.metas):
            n = min(len(self.texts), len(self.metas))
            print(f"[retriever] WARN: texts({len(self.texts)}) != metas({len(self.metas)}), trunc -> {n}")
            self.texts = self.texts[:n]
            self.metas = self.metas[:n]

        vec_pkl = d / "vectorizer.pkl"
        X_npz = d / "X_tfidf.npz"
        if vec_pkl.exists() and X_npz.exists():
            with vec_pkl.open("rb") as f:
                self.vectorizer = joblib.load(f)
            self.X = load_npz(X_npz)
            print(f"[retriever] loaded TF-IDF from disk, shape={self.X.shape}, time={time.time()-t0:.2f}s")
        else:
            self.vectorizer = TfidfVectorizer(
                lowercase=True,
                token_pattern=self.cfg.token_pattern,
                ngram_range=self.cfg.ngram_range,
                min_df=self.cfg.min_df,
                max_df=self.cfg.max_df,
                max_features=self.cfg.max_features,
                use_idf=True,
                smooth_idf=True,
                sublinear_tf=self.cfg.sublinear_tf,
                norm=self.cfg.norm,
            )
            self.X = self.vectorizer.fit_transform(self.texts)
            print(f"[retriever] fit TF-IDF, shape={self.X.shape}, time={time.time()-t0:.2f}s")

        # dense
        if self.use_dense:
            e1 = d / "E_dense.npy"
            if e1.exists():
                self.E = np.load(e1).astype("float32")
                print(f"[retriever] loaded E_dense.npy shape={self.E.shape}")
            elif _HAS_ST:
                try:
                    self._dense_model = SentenceTransformer("BAAI/bge-small-en-v1.5")
                    self.E = self._dense_model.encode(
                        self.texts, normalize_embeddings=True,
                        batch_size=16, show_progress_bar=False
                    ).astype("float32")
                    print(f"[retriever] computed dense, shape={self.E.shape}")
                except Exception as e:
                    print("[retriever] dense disabled:", e)
                    self.E = None
            else:
                self.E = None
        return self

    def _compute_priors(self) -> np.ndarray:
        now = _now_year()
        w = np.zeros(len(self.metas), float)
        for i, m in enumerate(self.metas):
            dt = str(m.get("doc_type", "other")).lower()
            w_dt = self.cfg.doc_type_weight.get(dt, self.cfg.doc_type_weight["other"])
            yr = m.get("year")
            if isinstance(yr, (int, float)) and yr > 0:
                age = max(0.0, now - float(yr))
                w_rec = math.exp(-age / max(1e-6, self.cfg.recency_tau))
            else:
                w_rec = 0.5
            bonus = 0.0
            if bool(m.get("pediatric")):
                bonus += self.cfg.pediatric_bonus
            if bool(m.get("single_ventricle")):
                bonus += self.cfg.sv_bonus
            w[i] = w_dt * w_rec + bonus
        return _minmax01(w)

    def _score_query(self, q: str, topk: int) -> Tuple[np.ndarray, np.ndarray]:
        assert self.vectorizer is not None and self.X is not None
        slex = (self.X @ self.vectorizer.transform([q]).T).toarray().ravel()
        slex = _minmax01(slex)
        if self.E is not None:
            if self._dense_model is None and _HAS_ST:
                self._dense_model = SentenceTransformer("BAAI/bge-small-en-v1.5")
            if self._dense_model is not None:
                qv = self._dense_model.encode(
                    [q], normalize_embeddings=True, show_progress_bar=False
                )[0]
                sdense = _minmax01(self.E @ qv)
            else:
                sdense = np.zeros_like(slex)
        else:
            sdense = np.zeros_like(slex)
        sprior = self._compute_priors()
        fused = (
            self.cfg.alpha_lex * slex
            + self.cfg.beta_dense * sdense
            + self.cfg.lambda_prior * sprior
        )
        idx = np.argsort(-fused)[:max(topk, self.cfg.max_keep)]
        return idx, fused[idx]

    def _variants(self, q: str) -> List[str]:
        v = [q.strip()]
        ql = q.lower()
        for k, exps in self.cfg.synonyms.items():
            if k in ql:
                for e in exps:
                    v.append(ql.replace(k, e))
        toks = re.findall(r"[\w/%.+\-]+", ql)
        drop = {
            "the", "a", "an", "for", "of", "to", "and",
            "or", "in", "on", "with", "without", "from", "by",
        }
        keep = [t for t in toks if t not in drop and len(t) > 1]
        v.append(" ".join(keep))
        uniq, seen = [], set()
        for s in v:
            s = re.sub(r"\s+", " ", s).strip()
            if s and s not in seen:
                uniq.append(s)
                seen.add(s)
        return uniq[:6]

    def _adaptive_cut(self, sorted_scores: np.ndarray) -> int:
        arr = np.asarray(sorted_scores, float)
        n = len(arr)
        if n == 0:
            return 0
        if np.all(arr <= 0):
            return max(self.cfg.min_keep, 1)
        for i in range(min(n - 1, self.cfg.max_keep * 2)):
            a, b = arr[i], arr[i + 1]
            rel = (a - b) / max(a, 1e-9)
            if rel >= self.cfg.adaptive_gamma and i + 1 >= self.cfg.min_keep:
                return min(i + 1, self.cfg.max_keep)
        return min(max(self.cfg.min_keep, n), self.cfg.max_keep)

    def _compress_mmr(self, doc_ids: np.ndarray, m: int) -> List[int]:
        ids = list(map(int, doc_ids))
        if len(ids) <= m:
            return ids
        dense = self.E is not None
        if dense:
            M = self.E[ids]
        else:
            M = self.X[ids].astype("float32")

        rel = np.linspace(1.0, 0.5, num=len(ids))
        selected = []
        cand = list(range(len(ids)))
        first = cand.pop(0)
        selected.append(first)

        def sim(i: int, j: int) -> float:
            if dense:
                return _cosine(M[i], M[j])
            num = (M[i] @ M[j].T).toarray().ravel()[0]
            den = (
                np.linalg.norm(M[i].toarray())
                * np.linalg.norm(M[j].toarray())
                + 1e-9
            )
            return float(num / den)

        while cand and len(selected) < m:
            best, best_score = None, -1e9
            for ci in cand:
                s_rel = rel[ci]
                max_sim = max(sim(ci, sj) for sj in selected)
                score = self.cfg.mmr_lambda * s_rel - (
                    1 - self.cfg.mmr_lambda
                ) * max_sim
                if score > best_score:
                    best_score = score
                    best = ci
            selected.append(best)
            cand.remove(best)
        return [ids[i] for i in selected]

    def search(self, query: str, max_return: Optional[int] = None) -> List[Dict[str, Any]]:
        assert self.X is not None
        rrf = np.zeros(self.X.shape[0], float)
        for qv in self._variants(query):
            idx, _ = self._score_query(qv, topk=self.cfg.variants_topk)
            for rank, did in enumerate(idx, start=1):
                rrf[did] += 1.0 / (self.cfg.rrf_k + rank)
        order = np.argsort(-rrf)
        keep = self._adaptive_cut(rrf[order])
        order = order[:keep]
        final_ids = self._compress_mmr(order, self.cfg.mmr_top_m)
        out = []
        for i, did in enumerate(final_ids, start=1):
            out.append({
                "rank": i,
                "doc_id": int(did),
                "score_rrf": float(rrf[did]),
                "text": self.texts[did],
                "meta": dict(self.metas[did]),
            })
        if max_return is not None:
            out = out[:max_return]
        return out

# ======================= Guideline nuggets =======================

def guideline_bullets(hits: List[Dict[str, Any]], char_limit: int = 1200) -> str:
    out: List[str] = []
    for h in hits:
        t = h["text"] or ""
        keep_lines = []
        for ln in t.splitlines():
            L = ln.strip()
            if not L:
                continue
            low = L.lower()
            if any(k in low for k in [
                "ecmo", "oxygenation index", "paO2", "fio2",
                "lactate", "single ventricle", "neonate",
                "infant", "pediatric", "criteria", "threshold",
                "nirs", "sv physiology",
            ]):
                keep_lines.append(L)
        if keep_lines:
            m = h["meta"]
            cite = f"[{h['rank']}] {m.get('source', m.get('file_name',''))}, {m.get('year','')}, p.{m.get('page','')}"
            out.append(cite + "\n- " + "\n- ".join(keep_lines[:6]))
        if sum(len(x) for x in out) > char_limit:
            break
    return "\n\n".join(out[:4])

# ======================= Patient State Card =======================

VITALS_CANON = ["HR", "RR", "O2_sat", "NIRS", "SBP", "DBP", "CVRP", "CVP"]
LABS = ["pH", "BE", "PAO2", "Lactate", "CR", "BUN", "MVO2_sats"]

def slope_per_hour(series: pd.Series, freq_sec: int) -> float:
    s = pd.to_numeric(series, errors="coerce").dropna().to_numpy()
    if len(s) < 2:
        return 0.0
    y0, y1 = s[0], s[-1]
    mins = (len(series) - 1) * (freq_sec / 60.0)
    if mins <= 0:
        return 0.0
    return (y1 - y0) / (mins / 60.0)

def last_minutes_window(df: pd.DataFrame, time_col: str, minutes: int, freq_sec: int) -> pd.DataFrame:
    df = df.copy()
    df[time_col] = pd.to_datetime(df[time_col])
    df = df.sort_values(time_col)
    step_min = freq_sec / 60.0
    steps = max(1, int(round(minutes / step_min)))
    return df.iloc[-steps:].reset_index(drop=True)

def lab_last_and_age(df: pd.DataFrame, time_col: str) -> List[str]:
    out = []
    now_ts = pd.to_datetime(df[time_col].iloc[-1])
    for lab in LABS:
        if lab not in df.columns:
            continue
        s = pd.to_numeric(df[lab], errors="coerce")
        s = s.dropna()
        if s.empty:
            continue
        last_val = float(s.iloc[-1])
        last_idx = s.index[-1]
        last_ts = pd.to_datetime(df.loc[last_idx, time_col])
        age_min = (now_ts - last_ts).total_seconds() / 60.0
        out.append(f"{lab}={last_val:.3g} (last {age_min:.0f} min ago)")
    return out

def get_low_high(var: str) -> Tuple[Optional[float], Optional[float]]:
    key = "CVRP" if var.upper() in ["CVP", "CVRP"] else var
    low = THRESHOLDS.get(f"{key}_low")
    high = THRESHOLDS.get(f"{key}_high")
    return low, high

def margin_str(var: str, val: float) -> Optional[str]:
    low, high = get_low_high(var)
    parts = []
    if low is not None:
        parts.append(f"to_low({low})={val - low:.2f}")
    if high is not None:
        parts.append(f"to_high({high})={high - val:.2f}")
    return "; ".join(parts) if parts else None

def make_patient_state_card(df: pd.DataFrame,
                            time_col: str,
                            freq_sec: int,
                            last_minutes: int = 60) -> Tuple[str, Dict[str, Any]]:
    df = df.copy()
    df[time_col] = pd.to_datetime(df[time_col])
    df = df.sort_values(time_col)
    win = last_minutes_window(df, time_col, last_minutes, freq_sec)

    def pick_col(name: str) -> Optional[str]:
        if name in win.columns:
            return name
        if name == "CVP" and "CVRP" in win.columns:
            return "CVRP"
        if name == "CVRP" and "CVP" in win.columns:
            return "CVP"
        return None

    lines = []
    stats: Dict[str, Any] = {}
    for name in VITALS_CANON:
        col = pick_col(name)
        if not col:
            continue
        w = pd.to_numeric(win[col], errors="coerce")
        if w.notna().sum() == 0:
            continue
        last = float(w.iloc[-1])
        vmin = float(np.nanmin(w))
        vmax = float(np.nanmax(w))
        mean = float(np.nanmean(w))
        slp = slope_per_hour(w, freq_sec)
        mg = margin_str(col, last)
        txt = f"{name}: last={last:.1f}, mean={mean:.1f}, min/max={vmin:.1f}/{vmax:.1f}, slope/h={slp:+.2f}"
        if mg:
            txt += f", margin({mg})"
        lines.append(txt)
        stats[name] = {
            "last": last,
            "mean": mean,
            "min": vmin,
            "max": vmax,
            "slope_per_h": slp,
            "margin": mg,
        }

    labs = lab_last_and_age(df, time_col)
    if labs:
        lines.append("Labs: " + "; ".join(labs))

    card = f"Patient state (last {last_minutes} min):\n- " + "\n- ".join(lines)
    return card, {"vitals": stats, "labs": labs}

# ======================= Hazard model & alarm =======================

def load_hazard_model():
    if not Path("hazard_model.pkl").exists():
        raise SystemExit("hazard_model.pkl 不存在，请先运行 predict_risk_baseline_3.py train")
    if not Path("hazard_model_features.json").exists():
        raise SystemExit("hazard_model_features.json 不存在，请先运行 predict_risk_baseline_3.py train")
    clf = joblib.load("hazard_model.pkl")
    feat_cols = json.loads(Path("hazard_model_features.json").read_text(encoding="utf-8"))

    thr_ref = 0.05
    thr_path = Path("hazard_threshold.json")
    if thr_path.exists():
        try:
            thr_data = json.loads(thr_path.read_text(encoding="utf-8"))
            if "hazard_threshold" in thr_data:
                thr_ref = float(thr_data["hazard_threshold"])
        except Exception:
            print("[warn] no hazard_threshold.json, use default 0.05")
    else:
        print("[warn] hazard_threshold.json not found, use default thr=0.3")

    return clf, feat_cols, thr_ref

def predict_proba_or_score(clf, X: pd.DataFrame) -> np.ndarray:
    if hasattr(clf, "predict_proba"):
        return clf.predict_proba(X)[:, 1]
    else:
        return clf.decision_function(X)

def ewma(x: np.ndarray, a: float) -> np.ndarray:
    y = np.zeros_like(x, float)
    m = 0.0
    for i, v in enumerate(x):
        m = a * v + (1 - a) * m
        y[i] = m
    return y

def alarm_from_series(proba: np.ndarray,
                      thr: float,
                      freq_sec: int,
                      alpha: float,
                      hold_min: float,
                      ref_min: float) -> Tuple[bool, np.ndarray]:
    smooth = ewma(proba, alpha) if 0 < alpha < 1 else proba
    step_min = freq_sec / 60.0
    hold = max(1, int(round(hold_min / step_min)))
    ref = max(1, int(round(ref_min / step_min)))
    above = smooth >= thr
    run = np.zeros_like(above, int)
    c = 0
    for i, f in enumerate(above):
        c = c + 1 if f else 0
        run[i] = c
    raw = run >= hold
    mask = np.zeros_like(raw, bool)
    i = 0
    while i < len(raw):
        if raw[i]:
            mask[i] = True
            i += ref
        else:
            i += 1
    return bool(mask[-1]), smooth

# ======================= LLM Prompt =======================

LLM_PROMPT = """You are a clinical assistant for pediatric cardiac ICU.

[QUESTION]
{question}

[PATIENT STATE CARD]
{patient_card}

[GUIDELINE NUGGETS]
{guideline}

[MODEL HAZARD SUMMARY]
- Latest hazard probability for next {H} minutes: {p_latest:.1%}
- Reference threshold: {thr:.3f}
- Alarm policy: EWMA(alpha={alpha}), hold={hold} min, refractory={ref} min
- Alarm_now (from model policy): {alarm_flag}

Return ONLY a JSON object:
{{
  "risk_level": "low" | "medium" | "high",
  "trigger_alarm_now": true | false,
  "key_reasons": ["bullet1","bullet2","..."],
  "suggested_actions": ["action1","action2","..."]
}}
Use concise, evidence-based reasoning consistent with pediatric single-ventricle physiology.
"""

TEMPLATE_FALLBACK = """# Risk & Guidance (template; LLM offline)

Latest hazard probability (next {H} min): {p_latest:.1%} (ref_thr={thr:.3f})
Alarm state (EWMA alpha={alpha}, hold={hold}min, refractory={ref}min): {alarm_flag}

## Patient state (last window)
{patient_card}

## Guideline nuggets
{guideline}
"""

# ======================= MAIN =======================

def ensure_outdir() -> Path:
    out = Path("out")
    out.mkdir(exist_ok=True)
    return out

def main():
    args = get_args()
    outdir = ensure_outdir()

    cfg = RetrievalConfig()
    retr = RetrieverStandalone(cfg, use_dense=args.use_dense).load_from_dir(Path(args.corpus))
    hits = retr.search(args.question, max_return=args.topk)
    (outdir / "retrieved_rag.json").write_text(
        json.dumps(hits, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )
    nuggets = guideline_bullets(hits)

    clf, feat_cols, thr_ref = load_hazard_model()

    df_full = pd.read_csv(args.patient_csv)
    feats, y_hazard, y_now = build_features_single(
        df=df_full,
        time_col=args.time_col,
        freq_sec=args.freq_sec,
        lead_min=args.lead_min,
        cooldown_min=args.cooldown_min,
    )

    X = feats.copy()
    for c in feat_cols:
        if c not in X.columns:
            X[c] = 0.0
    X = X[feat_cols]

    proba = predict_proba_or_score(clf, X)
    ts = X.index.to_numpy()
    risk_df = pd.DataFrame({
        "time": ts.astype(str),
        "hazard_prob": proba,
        "hazard_label": y_hazard.loc[X.index].values,
        "label_now": y_now[-len(X):],
    })
    risk_df.to_csv(outdir / "risk_series.csv", index=False)

    card, _ = make_patient_state_card(df_full, args.time_col, args.freq_sec, last_minutes=60)

    alarm_now, smooth = alarm_from_series(
        proba,
        thr_ref,
        args.freq_sec,
        args.smooth_alpha,
        args.hold_min,
        args.refractory_min,
    )
    p_latest = float(proba[-1])
    alarm_flag = "ON" if alarm_now else "OFF"

    final_text = ""
    decision_json = None

    if args.ollama_model:
        prompt = LLM_PROMPT.format(
            question=args.question,
            patient_card=card,
            guideline=nuggets,
            H=args.horizon_min,
            p_latest=p_latest,
            thr=thr_ref,
            alpha=args.smooth_alpha,
            hold=args.hold_min,
            ref=args.refractory_min,
            alarm_flag=alarm_flag,
        )
        raw = try_ollama(prompt, args.ollama_model)
        decision_json = extract_json(raw) if raw else None

        if decision_json:
            final_text = (
                f"# LLM Decision\n"
                f"- risk_level: **{decision_json.get('risk_level', '?')}**\n"
                f"- trigger_alarm_now: **{decision_json.get('trigger_alarm_now', False)}**\n"
                f"- key_reasons:\n  - " +
                "\n  - ".join(decision_json.get("key_reasons", [])[:6]) +
                "\n- suggested_actions:\n  - " +
                "\n  - ".join(decision_json.get("suggested_actions", [])[:6]) +
                f"\n\n## Patient state (last 60min)\n{card}\n\n"
                f"## Guideline nuggets\n{nuggets}\n"
                f"\n[ref] hazard_p_latest={p_latest:.1%}, thr_ref={thr_ref:.3f}, alarm_now={alarm_flag}"
            )
        else:
            final_text = TEMPLATE_FALLBACK.format(
                H=args.horizon_min,
                p_latest=p_latest,
                thr=thr_ref,
                alpha=args.smooth_alpha,
                hold=args.hold_min,
                ref=args.refractory_min,
                alarm_flag=alarm_flag,
                patient_card=card,
                guideline=nuggets,
            )
    else:
        final_text = TEMPLATE_FALLBACK.format(
            H=args.horizon_min,
            p_latest=p_latest,
            thr=thr_ref,
            alpha=args.smooth_alpha,
            hold=args.hold_min,
            ref=args.refractory_min,
            alarm_flag=alarm_flag,
            patient_card=card,
            guideline=nuggets,
        )

    (outdir / "final_answer_llm.txt").write_text(final_text, encoding="utf-8")
    if decision_json:
        (outdir / "llm_decision.json").write_text(
            json.dumps(decision_json, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

    print(final_text)


if __name__ == "__main__":
    main()




